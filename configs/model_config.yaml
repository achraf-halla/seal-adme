# SEAL-ADME Model and Training Configuration

# Model architecture
model:
  encoder_type: gcn          # gcn or gin
  input_features: 25         # Node feature dimension (from graph_featurizer)
  hidden_features: 256       # Hidden dimension
  num_layers: 4              # Number of GNN layers
  dropout: 0.1               # Dropout rate

# Regularization
regularization:
  encoder: 1.0e-4            # L1 penalty on inter-fragment weights
  contribution: 0.5          # L1 penalty on fragment contributions

# Pretraining configuration (multi-task classification)
pretrain:
  epochs: 50
  batch_size: 64
  learning_rate: 1.0e-3
  weight_decay: 1.0e-5
  
  # Balanced sampling ensures equal representation of all tasks
  sampling: balanced

# Finetuning configuration (single-task regression)
finetune:
  epochs: 150
  batch_size: 64
  learning_rate: 3.0e-4
  weight_decay: 1.0e-5
  
  # Early stopping
  patience: 20
  
  # Whether to freeze encoder during finetuning
  freeze_encoder: false

# Output
output:
  save_interval: 10          # Save checkpoint every N epochs
  log_interval: 1            # Log metrics every N epochs
